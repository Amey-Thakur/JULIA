{
 "cells": [
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Pkg; Pkg.add(Pkg.PackageSpec(url=\"https://github.com/JuliaComputing/JuliaAcademyData.jl\"))\n",
    "using JuliaAcademyData; activate(\"Deep learning with Flux\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "<br/>\n",
    "\n",
    "# Intro to Flux.jl"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "In the previous course, we learned how machine learning allows us to classify data as apples or bananas with a single neuron. However, some of those details are pretty fiddly! Fortunately, Julia has a powerful package that does much of the heavy lifting for us, called [`Flux.jl`](https://fluxml.github.io/).\n",
    "\n",
    "*Using `Flux` will make classifying data and images much easier!*"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Using `Flux.jl`\n",
    "\n",
    "We can get started with `Flux.jl` via:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# using Pkg; Pkg.add([\"Flux\", \"Plots\"])\n",
    "using Flux, Plots"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "#### Helpful built-in functions\n",
    "\n",
    "When working we'll `Flux`, we'll make use of built-in functionality that we've had to create for ourselves in previous notebooks.\n",
    "\n",
    "For example, the sigmoid function, σ, that we have been using already lives within `Flux`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "?σ"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(σ, -5, 5, label=\"\\\\sigma\", xlabel=\"x\", ylabel=\"\\\\sigma\\\\(x\\\\)\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Importantly, `Flux` allows us to *automatically create neurons* with the **`Dense`** function. For example, in the last notebook, we were looking at a neuron with 2 inputs and 1 output:\n",
    "\n",
    " <img src=\"https://raw.githubusercontent.com/JuliaComputing/JuliaAcademyData.jl/master/courses/Deep%20learning%20with%20Flux/data/single-neuron.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    " We could create a neuron with two inputs and one output via"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Dense(2, 1, σ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "This `model` object comes with places to store weights and biases:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model.W"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model.b"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "typeof(model.W)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x = rand(2)\n",
    "model(x)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "σ.(model.W*x + model.b)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Unlike in previous notebooks, note that `W` is no longer a `Vector` (1D `Array`) and `b` is no longer a number! Both are now stored in so-called `TrackedArray`s and `W` is effectively being treated as a matrix with a single row. We'll see why below."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Other helpful built-in functionality includes ways to automatically calculate gradients and also the cost function that we've used in the previous course -\n",
    "\n",
    "$$L(w, b) = \\sum_i \\left[y_i - f(x_i, w, b) \\right]^2$$\n",
    "\n",
    "If you normalize by dividing by the total number of elements, this becomes the \"mean square error\" function, which in `Flux` is named **`Flux.mse`**."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "methods(Flux.mse)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Bringing it all together\n",
    "\n",
    "Load the datasets that contain the features of the apple and banana images."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using CSV, DataFrames\n",
    "\n",
    "apples = DataFrame(CSV.File(datapath(\"data/apples.dat\"), delim='\\t', allowmissing=:none, normalizenames=true))\n",
    "bananas = DataFrame(CSV.File(datapath(\"data/bananas.dat\"), delim='\\t', allowmissing=:none, normalizenames=true));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "x_apples  = [ [row.red, row.green] for row in eachrow(apples)]\n",
    "x_bananas = [ [row.red, row.green] for row in eachrow(bananas)];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Concatenate the x (features) together to create a vector of all our datapoints, and create the corresponding vector of known labels:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xs = [x_apples; x_bananas]\n",
    "ys = [fill(0, size(x_apples)); fill(1, size(x_bananas))];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Dense(2, 1, σ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "We can evaluate the model (currently initialized with random weights) to see what the output value is for a given input:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model(xs[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "And of course we can examine the current loss value for that datapoint:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss = Flux.mse(model(xs[1]), ys[1])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "typeof(loss)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### Backpropagation"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model.W"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model.W.grad"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Flux.Tracker\n",
    "back!(loss)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model.W.grad"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "Now we have all the tools necessary to build a simple gradient descent algorithm!"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "### The easy way\n",
    "\n",
    "You don't want to manually write out gradient descent algorithms every time! Flux, of course, also brings in lots of optimizers that can do this all for you."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "?SGD"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "?Flux.train!"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "So we can simply define our loss function, an optimizer, and then call `train!`. That's basic machine learning with Flux.jl."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Dense(2, 1, σ)\n",
    "L(x,y) = Flux.mse(model(x), y)\n",
    "opt = SGD(params(model))\n",
    "Flux.train!(L, zip(xs, ys), opt)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "markdown",
   "source": [
    "## Visualize the result"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "contour(0:.1:1, 0:.1:1, (x, y) -> model([x,y])[].data, fill=true)\n",
    "scatter!(first.(x_apples), last.(x_apples), label=\"apples\")\n",
    "scatter!(first.(x_bananas), last.(x_bananas), label=\"bananas\")\n",
    "xlabel!(\"mean red value\")\n",
    "ylabel!(\"mean green value\")"
   ],
   "metadata": {},
   "execution_count": null
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  },
  "kernelspec": {
   "name": "julia-1.0",
   "display_name": "Julia 1.0.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
